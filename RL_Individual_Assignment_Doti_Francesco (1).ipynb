{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0Vw_-PFfvt3",
        "outputId": "1e114fe8-9637-4652-9b05-8f003b52bfb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gymnasium in /opt/anaconda3/lib/python3.12/site-packages (0.29.0)\n",
            "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (1.26.4)\n",
            "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.12/site-packages (3.9.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from gymnasium) (3.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from gymnasium) (4.11.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=8 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Collecting git+https://gitlab-research.centralesupelec.fr/stergios.christodoulidis/text-flappy-bird-gym.git\n",
            "  Cloning https://gitlab-research.centralesupelec.fr/stergios.christodoulidis/text-flappy-bird-gym.git to /private/var/folders/jj/5zz72tmj3wd9r01_nzqg4mv40000gn/T/pip-req-build-5s7iepko\n",
            "  Running command git clone --filter=blob:none --quiet https://gitlab-research.centralesupelec.fr/stergios.christodoulidis/text-flappy-bird-gym.git /private/var/folders/jj/5zz72tmj3wd9r01_nzqg4mv40000gn/T/pip-req-build-5s7iepko\n",
            "  Resolved https://gitlab-research.centralesupelec.fr/stergios.christodoulidis/text-flappy-bird-gym.git to commit ca2797e9270195313423324c9d0f205f6cbb3d28\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: gymnasium==0.29.0 in /opt/anaconda3/lib/python3.12/site-packages (from text-flappy-bird-gym==0.1.1) (0.29.0)\n",
            "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from text-flappy-bird-gym==0.1.1) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from gymnasium==0.29.0->text-flappy-bird-gym==0.1.1) (3.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from gymnasium==0.29.0->text-flappy-bird-gym==0.1.1) (4.11.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from gymnasium==0.29.0->text-flappy-bird-gym==0.1.1) (0.0.4)\n"
          ]
        }
      ],
      "source": [
        "# This script implements a Text Flappy Bird Reinforcement Learning setup\n",
        "# It merges improved training methods with a cleaner visualization approach\n",
        "\n",
        "# Install necessary libraries and the custom environment\n",
        "!pip install gymnasium numpy matplotlib\n",
        "!pip install git+https://gitlab-research.centralesupelec.fr/stergios.christodoulidis/text-flappy-bird-gym.git\n",
        "\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import the custom Flappy Bird text environment\n",
        "import text_flappy_bird_gym\n",
        "\n",
        "# Set random seeds to keep results reproducible\n",
        "np.random.seed(42)\n",
        "random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "yieJ8T4Wf0br"
      },
      "outputs": [],
      "source": [
        "def refine_state_view(screen):\n",
        "    \"\"\"\n",
        "    Convert the raw screen array into a more compact, meaningful state representation.\n",
        "    This helps the agent focus on critical information for the Flappy Bird environment.\n",
        "    \"\"\"\n",
        "    # Locate the bird\n",
        "    bird_positions = np.where(screen == 1)\n",
        "    if len(bird_positions[0]) > 0:\n",
        "        bird_x, bird_y = bird_positions[0][0], bird_positions[1][0]\n",
        "    else:\n",
        "        bird_x, bird_y = 0, 0\n",
        "\n",
        "    # Locate the pipes\n",
        "    pipe_positions = np.where(screen == 2)\n",
        "\n",
        "    # Construct a simplified view focusing on nearby pipes\n",
        "    if len(pipe_positions[0]) > 0:\n",
        "        pipe_x_coords = pipe_positions[0]\n",
        "        right_pipes = pipe_x_coords[pipe_x_coords > bird_x]\n",
        "\n",
        "        if len(right_pipes) > 0:\n",
        "            closest_pipe_x = np.min(right_pipes)\n",
        "            matching_y_coords = pipe_positions[1][pipe_x_coords == closest_pipe_x]\n",
        "\n",
        "            if len(matching_y_coords) > 0:\n",
        "                sorted_positions = np.sort(matching_y_coords)\n",
        "                if len(sorted_positions) > 1:\n",
        "                    # Identify the largest gap in the pipe\n",
        "                    vertical_gaps = np.diff(sorted_positions)\n",
        "                    biggest_gap_index = np.argmax(vertical_gaps)\n",
        "\n",
        "                    gap_bottom = sorted_positions[biggest_gap_index]\n",
        "                    gap_top = sorted_positions[biggest_gap_index + 1]\n",
        "                    gap_center = (gap_bottom + gap_top) // 2\n",
        "\n",
        "                    # Calculate distances for state representation\n",
        "                    horizontal_gap = closest_pipe_x - bird_x\n",
        "                    vertical_gap = gap_center - bird_y\n",
        "\n",
        "                    # Bin horizontal distance with a cap\n",
        "                    if horizontal_gap <= 3:\n",
        "                        horizontal_bin = horizontal_gap\n",
        "                    else:\n",
        "                        horizontal_bin = 3 + (horizontal_gap - 3) // 2\n",
        "                    horizontal_bin = min(8, horizontal_bin)\n",
        "\n",
        "                    # Bin vertical distance more precisely\n",
        "                    vertical_bin = max(-4, min(4, vertical_gap))\n",
        "\n",
        "                    # Player's absolute vertical location for edge considerations\n",
        "                    bird_centered_position = bird_y - (screen.shape[1] // 2)\n",
        "                    bird_centered_position = min(2, max(-2, bird_centered_position))\n",
        "\n",
        "                    return (horizontal_bin, vertical_bin, bird_centered_position)\n",
        "                else:\n",
        "                    # Partial pipe data was found\n",
        "                    return (\n",
        "                        min(8, (closest_pipe_x - bird_x)),\n",
        "                        0,\n",
        "                        min(2, max(-2, bird_y - screen.shape[1] // 2))\n",
        "                    )\n",
        "            else:\n",
        "                # No valid pipe y-coords found\n",
        "                return (min(8, 10), 0, min(2, max(-2, bird_y - screen.shape[1] // 2)))\n",
        "        else:\n",
        "            # All pipes are to the left, so it's presumably open\n",
        "            return (8, 0, min(2, max(-2, bird_y - screen.shape[1] // 2)))\n",
        "    else:\n",
        "        # No pipes are visible yet\n",
        "        return (8, 0, min(2, max(-2, bird_y - screen.shape[1] // 2)))\n",
        "\n",
        "\n",
        "def show_game_progress(env, reward=None, episode=None, step=None,\n",
        "                       total_reward=None, agent_type=None, action=None):\n",
        "    \"\"\"\n",
        "    Render the textual Flappy Bird environment with optional status details.\n",
        "    Kept here for demonstration replays, but removed from training loops.\n",
        "    \"\"\"\n",
        "    from IPython.display import clear_output\n",
        "    clear_output(wait=True)\n",
        "    game_state_str = env.render()\n",
        "\n",
        "    # Collect optional info\n",
        "    extra_info = []\n",
        "    if episode is not None:\n",
        "        extra_info.append(f\"Episode: {episode}\")\n",
        "    if step is not None:\n",
        "        extra_info.append(f\"Step: {step}\")\n",
        "    if reward is not None:\n",
        "        extra_info.append(f\"Reward: {reward}\")\n",
        "    if total_reward is not None:\n",
        "        extra_info.append(f\"Accumulated Score: {total_reward}\")\n",
        "    if agent_type is not None:\n",
        "        extra_info.append(f\"Agent: {agent_type}\")\n",
        "    if action is not None:\n",
        "        extra_info.append(f\"Action Taken: {'Flap' if action == 1 else 'Idle'}\")\n",
        "\n",
        "    if extra_info:\n",
        "        game_state_str += \"\\n\" + \" | \".join(extra_info)\n",
        "\n",
        "    print(game_state_str)\n",
        "    print(\"-\" * 50)\n",
        "    time.sleep(0.2)  # Slight delay for readability\n",
        "\n",
        "\n",
        "def plot_training_progress(scores, current_episode, label=\"\"):\n",
        "    \"\"\"\n",
        "    Plots the training scores (rewards) live after each episode.\n",
        "    \"\"\"\n",
        "    clear_output(True)  # Clears the previous plot\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(scores, label=f'{label} Rewards')\n",
        "    plt.title(f\"Training Progress - Episode {current_episode}\")\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Score\")\n",
        "    plt.grid(True)\n",
        "    if label:\n",
        "        plt.legend()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "BS0OlYH4f2pN"
      },
      "outputs": [],
      "source": [
        "class MonteCarloLearner:\n",
        "    \"\"\"\n",
        "    A Monte Carlo approach to learn a policy for the Text Flappy Bird game.\n",
        "    Uses first-visit MC with optimistic initialization.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, action_space_size, discount_factor=0.99, exploration_prob=0.2, initial_q_val=15.0):\n",
        "        self.action_space_size = action_space_size\n",
        "        self.discount_factor = discount_factor\n",
        "        self.exploration_prob = exploration_prob\n",
        "        self.min_epsilon = 0.001\n",
        "        self.epsilon_decay = 0.995\n",
        "\n",
        "        # Optimistic Q-array\n",
        "        self.Q = defaultdict(lambda: np.ones(action_space_size) * initial_q_val)\n",
        "        self.return_map = defaultdict(lambda: defaultdict(list))\n",
        "\n",
        "    def choose_action(self, state, allow_explore=True):\n",
        "        \"\"\"\n",
        "        Select an action by epsilon-greedy strategy.\n",
        "        \"\"\"\n",
        "        if allow_explore and random.random() < self.exploration_prob:\n",
        "            return random.randint(0, self.action_space_size - 1)\n",
        "        else:\n",
        "            return np.argmax(self.Q[state])\n",
        "\n",
        "    def adapt_exploration(self, current_episode, total_episodes):\n",
        "        \"\"\"\n",
        "        Adjust epsilon over time depending on training progress.\n",
        "        \"\"\"\n",
        "        if current_episode < total_episodes * 0.1:\n",
        "            self.exploration_prob = 0.5\n",
        "        elif current_episode < total_episodes * 0.3:\n",
        "            self.exploration_prob = 0.3\n",
        "        else:\n",
        "            self.exploration_prob = max(\n",
        "                self.min_epsilon,\n",
        "                0.2 * np.exp(-3.0 * current_episode / total_episodes)\n",
        "            )\n",
        "\n",
        "    def train_agent(self, env, num_episodes=1500, max_steps=1000):\n",
        "        \"\"\"\n",
        "        Train using Monte Carlo (first-visit). Episodes are generated and\n",
        "        Q-values are updated after each complete trajectory.\n",
        "        \"\"\"\n",
        "        episode_score_log = []\n",
        "        top_score = -float('inf')\n",
        "\n",
        "        for ep in range(1, num_episodes + 1):\n",
        "            # Update epsilon\n",
        "            self.adapt_exploration(ep, num_episodes)\n",
        "\n",
        "            # Generate a single trajectory (no console game printing)\n",
        "            state_log, action_log, reward_log = self.create_episode_trajectory(\n",
        "                env,\n",
        "                max_steps=max_steps\n",
        "            )\n",
        "\n",
        "            # Compute returns from the end to the beginning\n",
        "            G = 0\n",
        "            returns_list = []\n",
        "            for r in reversed(reward_log):\n",
        "                G = self.discount_factor * G + r\n",
        "                returns_list.insert(0, G)\n",
        "\n",
        "            # First-visit MC update\n",
        "            visited_pairs = set()\n",
        "            for idx, (obs, act) in enumerate(zip(state_log, action_log)):\n",
        "                if (obs, act) not in visited_pairs:\n",
        "                    visited_pairs.add((obs, act))\n",
        "                    self.return_map[obs][act].append(returns_list[idx])\n",
        "                    self.Q[obs][act] = np.mean(self.return_map[obs][act])\n",
        "\n",
        "            total_reward = sum(reward_log)\n",
        "            episode_score_log.append(total_reward)\n",
        "\n",
        "            # Live update plot each episode\n",
        "            plot_training_progress(episode_score_log, ep, label=\"MC\")\n",
        "\n",
        "            # Save model if this episode is the best so far\n",
        "            if total_reward > top_score:\n",
        "                top_score = total_reward\n",
        "                self.save_model('monte_carlo_agent_best.pkl')\n",
        "                print(f\"[MC] New best score: {top_score} (Episode {ep})\")\n",
        "\n",
        "            # Print progress at some intervals or final\n",
        "            if ep % 100 == 0 or ep == num_episodes:\n",
        "                avg_result = self.evaluate_performance(env, n_episodes=5)\n",
        "                print(f\"[MC] Episode {ep}/{num_episodes}, Epsilon: {self.exploration_prob:.4f}, \"\n",
        "                      f\"Avg Score: {avg_result:.2f}, Best: {top_score}\")\n",
        "\n",
        "        print(f\"[MC] Training Complete. Highest Score: {top_score}\")\n",
        "        return episode_score_log\n",
        "\n",
        "    def create_episode_trajectory(self, env, max_steps=1000):\n",
        "        \"\"\"\n",
        "        Generate one episode with the current policy. Logs states, actions, and rewards.\n",
        "        \"\"\"\n",
        "        state, _ = env.reset()\n",
        "        refined_state = refine_state_view(state)\n",
        "\n",
        "        state_log = []\n",
        "        action_log = []\n",
        "        reward_log = []\n",
        "\n",
        "        done = False\n",
        "        steps = 0\n",
        "\n",
        "        while not done and steps < max_steps:\n",
        "            chosen_action = self.choose_action(refined_state)\n",
        "            next_state, reward, terminated, truncated, _ = env.step(chosen_action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            next_refined_state = refine_state_view(next_state)\n",
        "\n",
        "            state_log.append(refined_state)\n",
        "            action_log.append(chosen_action)\n",
        "            reward_log.append(reward)\n",
        "\n",
        "            refined_state = next_refined_state\n",
        "            steps += 1\n",
        "\n",
        "        return state_log, action_log, reward_log\n",
        "\n",
        "    def evaluate_performance(self, env, n_episodes=5):\n",
        "        \"\"\"\n",
        "        Evaluate the trained policy without exploration for a few episodes.\n",
        "        Returns the average score.\n",
        "        \"\"\"\n",
        "        scores = []\n",
        "        for _ in range(n_episodes):\n",
        "            state, _ = env.reset()\n",
        "            refined_state = refine_state_view(state)\n",
        "            done = False\n",
        "            episode_score = 0\n",
        "            steps = 0\n",
        "\n",
        "            while not done and steps < 1000:\n",
        "                chosen_action = self.choose_action(refined_state, allow_explore=False)\n",
        "                next_state, reward, terminated, truncated, _ = env.step(chosen_action)\n",
        "                done = terminated or truncated\n",
        "                refined_state = refine_state_view(next_state)\n",
        "                episode_score += reward\n",
        "                steps += 1\n",
        "\n",
        "            scores.append(episode_score)\n",
        "\n",
        "        return np.mean(scores)\n",
        "\n",
        "    def save_model(self, filename):\n",
        "        \"\"\"Store the current Q-value dictionary in a pickle file.\"\"\"\n",
        "        with open(filename, 'wb') as f:\n",
        "            pickle.dump(dict(self.Q), f)\n",
        "\n",
        "    def load_model(self, filename):\n",
        "        \"\"\"\n",
        "        Load Q-values from a file and overwrite the current policy.\n",
        "        \"\"\"\n",
        "        with open(filename, 'rb') as f:\n",
        "            self.Q = defaultdict(lambda: np.zeros(self.action_space_size), pickle.load(f))\n",
        "            return True\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "xtE7ZZ30fr4e"
      },
      "outputs": [],
      "source": [
        "class SarsaEligibilityLearner:\n",
        "    \"\"\"\n",
        "    A Sarsa(λ) based agent that uses eligibility traces. Offers a more\n",
        "    incremental approach to training than Monte Carlo.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, action_space_size, discount_factor=0.99, learning_rate=0.1,\n",
        "                 trace_decay=0.9, exploration_prob=0.2, initial_q_val=15.0):\n",
        "        self.action_space_size = action_space_size\n",
        "        self.discount_factor = discount_factor\n",
        "        self.learning_rate = learning_rate\n",
        "        self.initial_learning_rate = learning_rate\n",
        "        self.trace_decay = trace_decay\n",
        "        self.exploration_prob = exploration_prob\n",
        "        self.min_epsilon = 0.001\n",
        "        self.epsilon_decay = 0.997\n",
        "\n",
        "        # Initialize Q with optimistic estimates\n",
        "        self.Q = defaultdict(lambda: np.ones(action_space_size) * initial_q_val)\n",
        "        self.eligibility_traces = defaultdict(lambda: np.zeros(action_space_size))\n",
        "\n",
        "    def choose_action(self, state, allow_explore=True):\n",
        "        \"\"\"\n",
        "        Choose an action via epsilon-greedy selection.\n",
        "        \"\"\"\n",
        "        if allow_explore and random.random() < self.exploration_prob:\n",
        "            return random.randint(0, self.action_space_size - 1)\n",
        "        else:\n",
        "            return np.argmax(self.Q[state])\n",
        "\n",
        "    def update_parameters(self, current_episode, total_episodes):\n",
        "        \"\"\"\n",
        "        Dynamically modify epsilon, alpha, and lambda as training advances.\n",
        "        \"\"\"\n",
        "        # Adjust epsilon\n",
        "        if current_episode < total_episodes * 0.1:\n",
        "            self.exploration_prob = 0.5\n",
        "        elif current_episode < total_episodes * 0.3:\n",
        "            self.exploration_prob = 0.3\n",
        "        else:\n",
        "            self.exploration_prob = max(\n",
        "                self.min_epsilon,\n",
        "                0.2 * np.exp(-3.0 * current_episode / total_episodes)\n",
        "            )\n",
        "\n",
        "        # Adjust alpha\n",
        "        self.learning_rate = max(\n",
        "            0.01,\n",
        "            self.initial_learning_rate * (1 - current_episode / total_episodes)\n",
        "        )\n",
        "\n",
        "        # Adjust lambda (trace decay)\n",
        "        self.trace_decay = 0.9 - 0.4 * (current_episode / total_episodes)\n",
        "\n",
        "    def reset_eligibility(self):\n",
        "        \"\"\"Reinitialize the eligibility traces for a fresh episode.\"\"\"\n",
        "        self.eligibility_traces = defaultdict(lambda: np.zeros(self.action_space_size))\n",
        "\n",
        "    def train_agent(self, env, num_episodes=1500, max_steps=1000):\n",
        "        \"\"\"\n",
        "        Train the Sarsa(λ) agent by interacting with the environment,\n",
        "        updating Q-values and traces at each step.\n",
        "        \"\"\"\n",
        "        episode_score_log = []\n",
        "        top_score = -float('inf')\n",
        "\n",
        "        for ep in range(1, num_episodes + 1):\n",
        "            self.update_parameters(ep, num_episodes)\n",
        "            self.reset_eligibility()\n",
        "\n",
        "            # Initial state-action\n",
        "            state, _ = env.reset()\n",
        "            refined_state = refine_state_view(state)\n",
        "            action = self.choose_action(refined_state)\n",
        "\n",
        "            done = False\n",
        "            total_reward = 0\n",
        "            steps = 0\n",
        "\n",
        "            while not done and steps < max_steps:\n",
        "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "                done = terminated or truncated\n",
        "                next_refined_state = refine_state_view(next_state)\n",
        "                next_action = self.choose_action(next_refined_state)\n",
        "\n",
        "                # TD Error\n",
        "                td_error = reward + (0 if done else\n",
        "                    self.discount_factor * self.Q[next_refined_state][next_action]) \\\n",
        "                    - self.Q[refined_state][action]\n",
        "\n",
        "                # Increase the trace for the current (state, action)\n",
        "                self.eligibility_traces[refined_state][action] += 1\n",
        "\n",
        "                # Update all Q values with non-zero trace\n",
        "                states_to_update = list(self.eligibility_traces.keys())\n",
        "                for s in states_to_update:\n",
        "                    for a_idx in range(self.action_space_size):\n",
        "                        if self.eligibility_traces[s][a_idx] > 0:\n",
        "                            self.Q[s][a_idx] += self.learning_rate * td_error * self.eligibility_traces[s][a_idx]\n",
        "                            self.eligibility_traces[s][a_idx] *= self.discount_factor * self.trace_decay\n",
        "\n",
        "                            # Prune very small traces\n",
        "                            if self.eligibility_traces[s][a_idx] < 0.01:\n",
        "                                self.eligibility_traces[s][a_idx] = 0\n",
        "\n",
        "                refined_state = next_refined_state\n",
        "                action = next_action\n",
        "                total_reward += reward\n",
        "                steps += 1\n",
        "\n",
        "            episode_score_log.append(total_reward)\n",
        "\n",
        "            # Live update the training plot\n",
        "            plot_training_progress(episode_score_log, ep, label=\"Sarsa(λ)\")\n",
        "\n",
        "            # Check for a best episode\n",
        "            if total_reward > top_score:\n",
        "                top_score = total_reward\n",
        "                self.save_model('sarsa_lambda_agent_best.pkl')\n",
        "                print(f\"[Sarsa(λ)] New best score: {top_score} (Episode {ep})\")\n",
        "\n",
        "            # Print summary every so often\n",
        "            if ep % 100 == 0 or ep == num_episodes:\n",
        "                avg_result = self.evaluate_performance(env, n_episodes=5)\n",
        "                print(f\"[Sarsa(λ)] Episode {ep}/{num_episodes}, \"\n",
        "                      f\"Epsilon: {self.exploration_prob:.4f}, \"\n",
        "                      f\"Alpha: {self.learning_rate:.4f}, \"\n",
        "                      f\"Lambda: {self.trace_decay:.2f}, \"\n",
        "                      f\"Avg Score: {avg_result:.2f}, Best: {top_score}\")\n",
        "\n",
        "        print(f\"[Sarsa(λ)] Training Complete. Highest Score: {top_score}\")\n",
        "        return episode_score_log\n",
        "\n",
        "    def evaluate_performance(self, env, n_episodes=10, render=False):\n",
        "        \"\"\"\n",
        "        Run the current policy (greedy) for a specified number of episodes,\n",
        "        returning the average total reward.\n",
        "        \"\"\"\n",
        "        scores = []\n",
        "\n",
        "        for ep in range(n_episodes):\n",
        "            state, _ = env.reset()\n",
        "            refined_state = refine_state_view(state)\n",
        "            done = False\n",
        "            episode_score = 0\n",
        "            steps = 0\n",
        "\n",
        "            while not done and steps < 1000:\n",
        "                action = self.choose_action(refined_state, allow_explore=False)\n",
        "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "                done = terminated or truncated\n",
        "                refined_state = refine_state_view(next_state)\n",
        "                episode_score += reward\n",
        "                steps += 1\n",
        "\n",
        "            scores.append(episode_score)\n",
        "\n",
        "        return np.mean(scores)\n",
        "\n",
        "    def save_model(self, filename):\n",
        "        \"\"\"Store learned Q-values for future use.\"\"\"\n",
        "        with open(filename, 'wb') as f:\n",
        "            pickle.dump(dict(self.Q), f)\n",
        "\n",
        "    def load_model(self, filename):\n",
        "        \"\"\"\n",
        "        Load previously stored Q-values and replace the current policy.\n",
        "        \"\"\"\n",
        "        with open(filename, 'rb') as f:\n",
        "            self.Q = defaultdict(lambda: np.zeros(self.action_space_size), pickle.load(f))\n",
        "            return True\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 740
        },
        "id": "Fy_Tababf_h6",
        "outputId": "6c33b2b2-d359-4a71-bf8e-62f5a5f788a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text Flappy Bird!\n",
            "Score: 99\n",
            "----------------------\n",
            "[         \u001b[32m|\u001b[0m          ]\n",
            "[         \u001b[32m|\u001b[0m          ]\n",
            "[         \u001b[32m|\u001b[0m          ]\n",
            "[         \u001b[32m|\u001b[0m          ]\n",
            "[         \u001b[32m|\u001b[0m          ]\n",
            "[                    ]\n",
            "[                    ]\n",
            "[                    ]\n",
            "[                    ]\n",
            "[      \u001b[33m@\u001b[0m  \u001b[32m|\u001b[0m          ]\n",
            "[         \u001b[32m|\u001b[0m          ]\n",
            "[         \u001b[32m|\u001b[0m          ]\n",
            "[         \u001b[32m|\u001b[0m          ]\n",
            "[         \u001b[32m|\u001b[0m          ]\n",
            "[         \u001b[32m|\u001b[0m          ]\n",
            "^^^^^^^^^^^^^^^^^^^^^^\n",
            "(Flap)\n",
            "\n",
            "Step: 999 | Reward: 1 | Accumulated Score: 999 | Agent: Sarsa-lambda | Action Taken: Idle\n",
            "--------------------------------------------------\n",
            "The best Sarsa-lambda run achieved a total reward of: 1000\n",
            "\n",
            "===== FINAL OUTCOME =====\n",
            "Monte Carlo Best Score: 1000\n",
            "Sarsa-lambda Best Score: 1000\n",
            "Absolute Difference: 0\n",
            "Both methods performed equally well.\n"
          ]
        }
      ],
      "source": [
        "def display_optimal_run(agent, env, agent_label, max_episodes=10, max_steps=1000):\n",
        "    \"\"\"\n",
        "    Execute several greedy episodes for a given agent,\n",
        "    track their cumulative rewards, and present the best-performing run.\n",
        "    \"\"\"\n",
        "    best_total = -float('inf')\n",
        "    best_states = []\n",
        "    best_actions = []\n",
        "    best_rewards = []\n",
        "\n",
        "    print(f\"Searching for the most successful run by {agent_label} across {max_episodes} trials...\")\n",
        "\n",
        "    for episode_index in range(max_episodes):\n",
        "        current_state, _ = env.reset()\n",
        "        episode_states = []\n",
        "        episode_actions = []\n",
        "        episode_rewards = []\n",
        "        done = False\n",
        "        steps_taken = 0\n",
        "        total_reward = 0\n",
        "\n",
        "        while not done and steps_taken < max_steps:\n",
        "            processed_state = refine_state_view(current_state)\n",
        "            chosen_action = agent.choose_action(processed_state, allow_explore=False)\n",
        "            next_state, reward, terminated, truncated, _ = env.step(chosen_action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            episode_states.append(current_state)\n",
        "            episode_actions.append(chosen_action)\n",
        "            episode_rewards.append(reward)\n",
        "\n",
        "            current_state = next_state\n",
        "            total_reward += reward\n",
        "            steps_taken += 1\n",
        "\n",
        "        print(f\"Episode {episode_index + 1} finished with reward = {total_reward}, steps = {steps_taken}\")\n",
        "\n",
        "        # Update if this run is the current best\n",
        "        if total_reward > best_total:\n",
        "            best_total = total_reward\n",
        "            best_states = episode_states\n",
        "            best_actions = episode_actions\n",
        "            best_rewards = episode_rewards\n",
        "            print(f\"New top performance recorded with total reward {total_reward}!\")\n",
        "\n",
        "    # Replay the top-performing episode\n",
        "    print(f\"\\nReplaying the best {agent_label} episode (final reward = {best_total})...\")\n",
        "\n",
        "    replay_score = 0\n",
        "    for index, (st, act, rew) in enumerate(zip(best_states, best_actions, best_rewards)):\n",
        "        replay_score += rew\n",
        "        # Render the textual environment every other step\n",
        "        if index % 2 == 0:\n",
        "            show_game_progress(\n",
        "                env,\n",
        "                reward=rew,\n",
        "                step=index + 1,\n",
        "                total_reward=replay_score,\n",
        "                action=act,\n",
        "                agent_type=agent_label\n",
        "            )\n",
        "\n",
        "    print(f\"The best {agent_label} run achieved a total reward of: {best_total}\")\n",
        "    return best_total\n",
        "\n",
        "\n",
        "def visualize_learning_progress(mc_scores, sarsa_scores, window_size=20):\n",
        "    \"\"\"\n",
        "    Provide a side-by-side view of the Monte Carlo vs. Sarsa-lambda\n",
        "    training progress, complete with rolling averages for clarity.\n",
        "    \"\"\"\n",
        "    def smooth_data(data, w_size):\n",
        "        # Simple rolling mean to reduce variance in the reward plots\n",
        "        if w_size > len(data):\n",
        "            w_size = len(data) // 2\n",
        "        if w_size < 1:\n",
        "            w_size = 1\n",
        "        cumulative_sum = np.cumsum(np.insert(data, 0, 0))\n",
        "        return (cumulative_sum[w_size:] - cumulative_sum[:-w_size]) / float(w_size)\n",
        "\n",
        "    mc_smoothed = smooth_data(mc_scores, window_size)\n",
        "    sarsa_smoothed = smooth_data(sarsa_scores, window_size)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    # Plot original results\n",
        "    plt.plot(mc_scores, label='MC (raw)', alpha=0.4)\n",
        "    plt.plot(sarsa_scores, label='Sarsa-lambda (raw)', alpha=0.4)\n",
        "\n",
        "    # Plot smoothed curves\n",
        "    plt.plot(range(window_size - 1, len(mc_scores)), mc_smoothed, linewidth=2,\n",
        "             label='MC (smoothed)')\n",
        "    plt.plot(range(window_size - 1, len(sarsa_scores)), sarsa_smoothed, linewidth=2,\n",
        "             label='Sarsa-lambda (smoothed)')\n",
        "\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Reward')\n",
        "    plt.title('MC vs. Sarsa-lambda Learning Progress')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.savefig('learning_curves.png')\n",
        "    plt.show()\n",
        "\n",
        "    # Print out a brief summary\n",
        "    print(\"\\n=== Summary of Training Results ===\")\n",
        "    print(f\"Monte Carlo  - Average over last 100 episodes: {np.mean(mc_scores[-100:]):.2f}\")\n",
        "    print(f\"Sarsa-lambda - Average over last 100 episodes: {np.mean(sarsa_scores[-100:]):.2f}\")\n",
        "    print(f\"Monte Carlo  - Peak reward: {np.max(mc_scores):.2f}\")\n",
        "    print(f\"Sarsa-lambda - Peak reward: {np.max(sarsa_scores):.2f}\")\n",
        "\n",
        "\n",
        "def execute_experiment(total_episodes=1500, load_existing=False):\n",
        "    \"\"\"\n",
        "    Main routine that sets up the environment, initializes both agents,\n",
        "    proceeds with training or model loading, and finally contrasts their performances.\n",
        "    \"\"\"\n",
        "    env = gym.make('TextFlappyBird-screen-v0', height=15, width=20, pipe_gap=4)\n",
        "\n",
        "    # Initialize two distinct approaches\n",
        "    mc_agent = MonteCarloLearner(\n",
        "        action_space_size=env.action_space.n,\n",
        "        discount_factor=0.99,\n",
        "        exploration_prob=0.2,\n",
        "        initial_q_val=15.0\n",
        "    )\n",
        "\n",
        "    sarsa_agent = SarsaEligibilityLearner(\n",
        "        action_space_size=env.action_space.n,\n",
        "        discount_factor=0.99,\n",
        "        learning_rate=0.1,\n",
        "        trace_decay=0.9,\n",
        "        exploration_prob=0.2,\n",
        "        initial_q_val=15.0\n",
        "    )\n",
        "\n",
        "    # If indicated, try to load previously stored policies\n",
        "    if load_existing:\n",
        "        try:\n",
        "            mc_agent.load_model('monte_carlo_agent_best.pkl')\n",
        "            print(\"Loaded existing Monte Carlo agent successfully.\")\n",
        "        except:\n",
        "            print(\"Monte Carlo model not found; new training will be conducted.\")\n",
        "            load_existing = False\n",
        "\n",
        "        try:\n",
        "            sarsa_agent.load_model('sarsa_lambda_agent_best.pkl')\n",
        "            print(\"Loaded existing Sarsa-lambda agent successfully.\")\n",
        "        except:\n",
        "            print(\"Sarsa-lambda model not found; new training will be conducted.\")\n",
        "            load_existing = False\n",
        "\n",
        "    # If models weren't loaded, run the training process\n",
        "    if not load_existing:\n",
        "        print(\"\\n===== TRAINING MONTE CARLO AGENT =====\")\n",
        "        mc_scores = mc_agent.train_agent(env, num_episodes=total_episodes)\n",
        "\n",
        "        env.close()\n",
        "        env = gym.make('TextFlappyBird-screen-v0', height=15, width=20, pipe_gap=4)\n",
        "\n",
        "        print(\"\\n===== TRAINING SARSA-LAMBDA AGENT =====\")\n",
        "        sarsa_scores = sarsa_agent.train_agent(env, num_episodes=total_episodes)\n",
        "\n",
        "        # Compare the two learning curves\n",
        "        visualize_learning_progress(mc_scores, sarsa_scores)\n",
        "    else:\n",
        "        # If we loaded models, define placeholders for the final comparison plot\n",
        "        # (You might have them stored somewhere else if you saved them.)\n",
        "        mc_scores = []\n",
        "        sarsa_scores = []\n",
        "\n",
        "    print(\"\\n===== DEMONSTRATIONS OF BEST RUNS =====\")\n",
        "\n",
        "    print(\"\\n--- Monte Carlo Method ---\")\n",
        "    mc_best_run = display_optimal_run(mc_agent, env, \"Monte Carlo\", max_episodes=10)\n",
        "\n",
        "    env.close()\n",
        "    env = gym.make('TextFlappyBird-screen-v0', height=15, width=20, pipe_gap=4)\n",
        "\n",
        "    print(\"\\n--- Sarsa-lambda Method ---\")\n",
        "    sarsa_best_run = display_optimal_run(sarsa_agent, env, \"Sarsa-lambda\", max_episodes=10)\n",
        "\n",
        "    print(\"\\n===== FINAL OUTCOME =====\")\n",
        "    print(f\"Monte Carlo Best Score: {mc_best_run}\")\n",
        "    print(f\"Sarsa-lambda Best Score: {sarsa_best_run}\")\n",
        "    print(f\"Absolute Difference: {abs(mc_best_run - sarsa_best_run)}\")\n",
        "    if mc_best_run > sarsa_best_run:\n",
        "        print(\"Monte Carlo outperformed Sarsa-lambda in these trials.\")\n",
        "    elif sarsa_best_run > mc_best_run:\n",
        "        print(\"Sarsa-lambda outperformed Monte Carlo in these trials.\")\n",
        "    else:\n",
        "        print(\"Both methods performed equally well.\")\n",
        "\n",
        "    env.close()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Adjust hyperparameters and settings below\n",
        "    EPISODES_TO_TRAIN = 2000\n",
        "    LOAD_MODELS = False\n",
        "\n",
        "    execute_experiment(total_episodes=EPISODES_TO_TRAIN, load_existing=LOAD_MODELS)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
